# Transformer Model Benchmarking Suite

ä¸€ä¸ªå…¨é¢çš„ Transformer å¤§æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•å¥—ä»¶ï¼Œæ”¯æŒæµ‹é‡ prefill timeã€decode timeï¼ˆæ¯ä¸ª tokenï¼‰å’Œ total latencyï¼Œå¯ä»¥æµ‹è¯•ä¸åŒ batch size å’Œ input length ç»„åˆä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **ç²¾ç¡®çš„æ€§èƒ½æµ‹é‡**: ä½¿ç”¨TTFTï¼ˆTime To First Tokenï¼‰å’Œper token decode timeå‡†ç¡®æµ‹é‡ prefill time å’Œ decode time
- ğŸ¯ **ç›´æ¥Tokenç”Ÿæˆ**: è·³è¿‡tokenizerç¼–ç /è§£ç è¿‡ç¨‹ï¼Œç›´æ¥ç”ŸæˆæŒ‡å®šé•¿åº¦çš„tokenåºåˆ—ï¼Œæ¶ˆé™¤æ–‡æœ¬å¤„ç†å¼€é”€
- ğŸ“Š **å¤šç»´åº¦æµ‹è¯•**: æ”¯æŒä¸åŒ batch size å’Œ input length çš„ç»„åˆæµ‹è¯•
- ğŸ“ˆ **è¯¦ç»†çš„æ•°æ®åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œç»Ÿè®¡åˆ†ææŠ¥å‘Š
- ğŸ”§ **çµæ´»é…ç½®**: æ”¯æŒå¤šç§é¢„è®¾é…ç½®å’Œè‡ªå®šä¹‰å‚æ•°
- ğŸ’¾ **å¤šæ ¼å¼è¾“å‡º**: æ”¯æŒ JSON å’Œ CSV æ ¼å¼çš„ç»“æœä¿å­˜
- ğŸ–¥ï¸ **è·¨å¹³å°æ”¯æŒ**: æ”¯æŒ CPU å’Œ GPUï¼ˆCUDAï¼‰æ¨ç†
- âš¡ **æ™ºèƒ½è·³è¿‡**: è‡ªåŠ¨è·³è¿‡è¶…å¤§é…ç½®ä»¥é¿å…å†…å­˜é—®é¢˜

## æ–‡ä»¶ç»“æ„

```
transformers_profile/
â”œâ”€â”€ benchmark.py          # ä¸»è¦çš„æ€§èƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ config.py            # é…ç½®ç®¡ç†æ¨¡å—
â”œâ”€â”€ analyze_results.py   # æ•°æ®åˆ†æå’Œå¯è§†åŒ–è„šæœ¬
â”œâ”€â”€ requirements.txt     # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md           # ä½¿ç”¨è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†æˆ–ä¸‹è½½é¡¹ç›®åˆ°æœ¬åœ°
cd transformers_profile

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. åŸºæœ¬ä½¿ç”¨

#### è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®æµ‹è¯• GPT-2 æ¨¡å‹
python benchmark.py --model gpt2 --device auto

# æŒ‡å®š GPU è®¾å¤‡
python benchmark.py --model gpt2 --device cuda:0

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python benchmark.py --model gpt2 --config-file custom_config.json

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶åå‰ç¼€
python benchmark.py --model gpt2 --output-file my_benchmark
```

#### ç”Ÿæˆé…ç½®æ–‡ä»¶

```bash
# æŸ¥çœ‹å¯ç”¨çš„é¢„è®¾é…ç½®
python config.py --show-presets

# ç”Ÿæˆå¿«é€Ÿæµ‹è¯•é…ç½®
python config.py --create quick_test --output quick_test_config.json

# ç”Ÿæˆä¸­ç­‰è§„æ¨¡æµ‹è¯•é…ç½®
python config.py --create medium_scale --output medium_config.json
```

#### åˆ†ææµ‹è¯•ç»“æœ

```bash
# åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ
python analyze_results.py benchmark_results_20241017_143022.json

# æŒ‡å®šè¾“å‡ºç›®å½•
python analyze_results.py results.json --output-dir analysis_output

# åªç”ŸæˆæŠ¥å‘Šï¼Œä¸ç”Ÿæˆå›¾è¡¨
python analyze_results.py results.json --report-only
```

## è¯¦ç»†ä½¿ç”¨æŒ‡å—

### é…ç½®å‚æ•°è¯´æ˜

#### åŸºæœ¬æµ‹è¯•å‚æ•°

- **batch_sizes**: æ‰¹å¤„ç†å¤§å°åˆ—è¡¨ï¼Œå¦‚ `[1, 2, 4, 8, 16, 32, 64, 128, 256]`
- **input_lengths**: è¾“å…¥åºåˆ—é•¿åº¦åˆ—è¡¨ï¼Œå¦‚ `[32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]`
- **output_length**: å›ºå®šè¾“å‡ºé•¿åº¦ï¼ˆæ‰€æœ‰æµ‹è¯•ä½¿ç”¨ç›¸åŒå€¼ï¼‰
- **num_runs**: æ¯ä¸ªé…ç½®é‡å¤æµ‹è¯•çš„æ¬¡æ•°ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
- **max_batch_input_product**: æœ€å¤§æ‰¹å¤„ç†Ã—è¾“å…¥é•¿åº¦ä¹˜ç§¯ï¼ˆé»˜è®¤131072ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼çš„å®éªŒå°†è¢«è·³è¿‡ä»¥é¿å…å†…å­˜é—®é¢˜

#### ç”Ÿæˆå‚æ•°

- **temperature**: é‡‡æ ·æ¸©åº¦ï¼ˆ0 è¡¨ç¤ºè´ªå¿ƒè§£ç ï¼Œ>0 è¡¨ç¤ºéšæœºé‡‡æ ·ï¼‰
- **top_p**: Nucleus é‡‡æ ·å‚æ•°

### é¢„è®¾é…ç½®

| é…ç½®åç§° | æ‰¹å¤„ç†å¤§å° | è¾“å…¥é•¿åº¦ | è¾“å‡ºé•¿åº¦ | è¿è¡Œæ¬¡æ•° | é€‚ç”¨åœºæ™¯ |
|---------|-----------|----------|----------|----------|----------|
| quick_test | [1, 2] | [32, 64] | 20 | 1 | å¿«é€ŸéªŒè¯ |
| small_scale | [1, 2] | [32, 64, 128] | 30 | 2 | å°è§„æ¨¡æµ‹è¯• |
| medium_scale | [1, 2, 4, 8, 16, 32] | [32, 64, 128, 256, 512, 1024, 2048] | 50 | 3 | æ ‡å‡†æµ‹è¯• |
| large_scale | [1, 2, 4, 8, 16, 32, 64, 128, 256] | [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536] | 100 | 5 | å¤§è§„æ¨¡æµ‹è¯• |
| batch_size_study | [1, 2, 4, 8, 16, 32, 64, 128, 256] | [256] | 50 | 5 | æ‰¹å¤„ç†è§„æ¨¡ç ”ç©¶ |
| input_length_study | [1] | [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536] | 50 | 5 | è¾“å…¥é•¿åº¦ç ”ç©¶ |

## è¾“å‡ºç»“æœè¯´æ˜

### æ€§èƒ½æŒ‡æ ‡

- **total_latency**: æ€»å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
- **prefill_time**: å‡†ç¡®çš„ prefill æ—¶é—´ï¼ˆTTFT - Time To First Tokenï¼‰ï¼ˆç§’ï¼‰
- **decode_time**: å‡†ç¡®çš„è§£ç æ—¶é—´ï¼ˆå‰©ä½™tokensçš„ç”Ÿæˆæ—¶é—´ï¼‰ï¼ˆç§’ï¼‰
- **decode_time_per_token**: æ¯ä¸ª token çš„è§£ç æ—¶é—´ï¼ˆç§’ï¼‰
- **decode_tokens_count**: ç”¨äºè®¡ç®—decodeæ—¶é—´çš„tokenæ•°é‡
- **tokens_per_second**: ååé‡ï¼ˆtokens/ç§’ï¼‰
- **memory_usage**: å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆCPU å’Œ GPUï¼‰

### è¾“å‡ºæ–‡ä»¶

1. **JSON æ ¼å¼** (`benchmark_results_YYYYMMDD_HHMMSS.json`)
   - å®Œæ•´çš„åŸå§‹æµ‹è¯•æ•°æ®
   - åŒ…å«æ‰€æœ‰é…ç½®å’Œè¿è¡Œä¿¡æ¯

2. **CSV æ ¼å¼** (`benchmark_results_YYYYMMDD_HHMMSS.csv`)
   - è¡¨æ ¼å½¢å¼çš„ç»“æœæ•°æ®
   - ä¾¿äº Excel ç­‰å·¥å…·åˆ†æ

3. **åˆ†ææŠ¥å‘Š** (`analysis_report.md`)
   - Markdown æ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š
   - åŒ…å«æ€§èƒ½æ€»ç»“å’Œå»ºè®®

4. **å¯è§†åŒ–å›¾è¡¨** (`analysis_plots/` ç›®å½•)
   - `latency_heatmap.png`: å»¶è¿Ÿçƒ­åŠ›å›¾
   - `throughput_heatmap.png`: ååé‡çƒ­åŠ›å›¾
   - `batch_size_scaling.png`: æ‰¹å¤„ç†å¤§å°æ‰©å±•æ€§åˆ†æ
   - `input_length_scaling.png`: è¾“å…¥é•¿åº¦æ‰©å±•æ€§åˆ†æ
   - `time_breakdown.png`: æ—¶é—´åˆ†è§£å›¾
   - `performance_comparison.png`: æ€§èƒ½å¯¹æ¯”å›¾
   - `memory_usage.png`: å†…å­˜ä½¿ç”¨åˆ†æï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®

åˆ›å»ºè‡ªå·±çš„é…ç½®æ–‡ä»¶ `my_config.json`ï¼š

```json
{
  "batch_sizes": [1, 4, 16, 64],
  "input_lengths": [128, 512, 2048, 8192],
  "output_length": 100,
  "num_runs": 3,
  "temperature": 0.7,
  "top_p": 0.95,
  "max_batch_input_product": 32768
}
```

ä½¿ç”¨è‡ªå®šä¹‰é…ç½®ï¼š

```bash
python benchmark.py --model your-model --config-file my_config.json
```

### æµ‹è¯•å¤šä¸ªæ¨¡å‹

```bash
# é¡ºåºæµ‹è¯•å¤šä¸ªæ¨¡å‹
for model in "gpt2" "distilgpt2" "microsoft/DialoGPT-small"
do
    python benchmark.py --model $model --output-file ${model//\//_}_results
done
```

### æ‰¹é‡åˆ†æç»“æœ

```bash
# åˆ†æå¤šä¸ªç»“æœæ–‡ä»¶
for result_file in *_results.json
do
    echo "Analyzing $result_file..."
    python analyze_results.py $result_file --output-dir "analysis_$(basename $result_file .json)"
done
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPU ä¼˜åŒ–

1. **ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹**: è„šæœ¬è‡ªåŠ¨ä½¿ç”¨ `float16` ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
2. **æ‰¹å¤„ç†ä¼˜åŒ–**: è¾ƒå¤§çš„ batch size é€šå¸¸æœ‰æ›´å¥½çš„ GPU åˆ©ç”¨ç‡
3. **å†…å­˜ç®¡ç†**: æµ‹è¯•ä¼šè‡ªåŠ¨æ¸…ç† GPU ç¼“å­˜ä»¥é¿å… OOM

### ç›´æ¥Tokenç”Ÿæˆ

æœ¬æµ‹è¯•å¥—ä»¶é‡‡ç”¨ **ç›´æ¥tokenç”Ÿæˆ** çš„å…ˆè¿›æ–¹æ³•ï¼š

- **é›¶å¼€é”€ç”Ÿæˆ**: ç›´æ¥ç”Ÿæˆtoken IDï¼Œæ— éœ€tokenizerç¼–ç /è§£ç è¿‡ç¨‹
- **ç²¾ç¡®é•¿åº¦æ§åˆ¶**: æ¯ä¸ªè¾“å…¥åºåˆ—éƒ½æœ‰**ä¸¥æ ¼**çš„æŒ‡å®štokenæ•°é‡
- **æ‰¹æ¬¡å¤šæ ·æ€§**: æ¯ä¸ªbatché¡¹ç›®ä½¿ç”¨ä¸åŒçš„tokenæ¨¡å¼ä»¥ç¡®ä¿å¤šæ ·æ€§
- **è¯æ±‡è¡¨å…¼å®¹**: è‡ªåŠ¨ç¡®ä¿æ‰€æœ‰ç”Ÿæˆçš„tokenéƒ½åœ¨æ¨¡å‹è¯æ±‡è¡¨èŒƒå›´å†…

```python
# ç¤ºä¾‹ï¼šç›´æ¥ç”Ÿæˆtoken tensor
input_tokens = benchmark.generate_input_tokens(batch_size=4, input_length=256)
print(f"Shape: {input_tokens.shape}")  # torch.Size([4, 256])
print(f"Data type: {input_tokens.dtype}")  # torch.int64
# æ¯ä¸ªåºåˆ—éƒ½æ°å¥½æ˜¯256ä¸ªtokensï¼Œå¯ç›´æ¥è¾“å…¥æ¨¡å‹
```

**ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”ï¼š**

| ç‰¹æ€§ | ä¼ ç»Ÿæ–‡æœ¬æ–¹æ³• | ç›´æ¥Tokenæ–¹æ³• |
|------|-------------|---------------|
| ç”Ÿæˆé€Ÿåº¦ | æ…¢ï¼ˆéœ€è¦ç¼–ç /è§£ç ï¼‰ | **å¿«ï¼ˆç›´æ¥ç”Ÿæˆï¼‰** |
| é•¿åº¦ç²¾åº¦ | è¿‘ä¼¼ï¼ˆä¾èµ–tokenizerï¼‰ | **100%ç²¾ç¡®** |
| å¤„ç†å¼€é”€ | é«˜ï¼ˆå¤šæ¬¡è½¬æ¢ï¼‰ | **é›¶å¼€é”€** |
| è·¨æ¨¡å‹ä¸€è‡´æ€§ | ä¾èµ–tokenizerå·®å¼‚ | **å®Œå…¨ä¸€è‡´** |
| å¤§é•¿åº¦æ”¯æŒ | å—é™äºæ–‡æœ¬å¤„ç† | **æ— é™åˆ¶æ”¯æŒ** |

### æµ‹è¯•æœ€ä½³å®è·µ

1. **çƒ­èº«**: è„šæœ¬è‡ªåŠ¨è¿›è¡Œæ¨¡å‹çƒ­èº«ä»¥ç¡®ä¿ç¨³å®šçš„æ—¶é—´æµ‹é‡
2. **å¤šæ¬¡è¿è¡Œ**: ä½¿ç”¨ `num_runs > 1` è·å¾—æ›´å¯é çš„å¹³å‡å€¼
3. **ç³»ç»Ÿç›‘æ§**: ç›‘æ§ CPU å’Œ GPU ä½¿ç”¨æƒ…å†µä»¥ç¡®ä¿æ— å…¶ä»–è¿›ç¨‹å¹²æ‰°
4. **å†…å­˜é™åˆ¶**: ç³»ç»Ÿä¼šè‡ªåŠ¨è·³è¿‡ `batch_size Ã— input_length` è¶…è¿‡é˜ˆå€¼çš„å®éªŒä»¥é¿å…å†…å­˜æº¢å‡º
5. **ç›´æ¥Tokenè¾“å…¥**: ä½¿ç”¨ç›´æ¥ç”Ÿæˆçš„tokenåºåˆ—ï¼Œæ¶ˆé™¤tokenizerå¤„ç†çš„å˜å¼‚æ€§å’Œå¼€é”€

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æµ‹è¯•æœ¬åœ°æ¨¡å‹ï¼Ÿ
```bash
python benchmark.py --model /path/to/your/local/model --device auto
```

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
- å‡å° batch_sizes å’Œ input_lengths
- ä½¿ç”¨ `--device cpu` è¿›è¡Œ CPU æµ‹è¯•
- ä½¿ç”¨ `quick_test` æˆ– `small_scale` é…ç½®

### Q: å¦‚ä½•åªæµ‹è¯•ç‰¹å®šé…ç½®ï¼Ÿ
åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼ŒåªåŒ…å«éœ€è¦çš„å‚æ•°ç»„åˆã€‚

### Q: ç»“æœä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ
- å¢åŠ  `num_runs` å€¼
- ç¡®ä¿ç³»ç»Ÿè´Ÿè½½è¾ƒä½
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»– GPU è¿›ç¨‹è¿è¡Œ

## ç¤ºä¾‹è„šæœ¬

### å®Œæ•´æµ‹è¯•æµç¨‹

```bash
#!/bin/bash

# 1. åˆ›å»ºé…ç½®æ–‡ä»¶
python config.py --create medium_scale --output test_config.json

# 2. è¿è¡ŒåŸºå‡†æµ‹è¯•
python benchmark.py --model gpt2 --config-file test_config.json --output-file gpt2_benchmark

# 3. åˆ†æç»“æœ
python analyze_results.py gpt2_benchmark.json --output-dir gpt2_analysis

echo "æµ‹è¯•å®Œæˆï¼æŸ¥çœ‹ gpt2_analysis/ ç›®å½•è·å–è¯¦ç»†åˆ†æç»“æœã€‚"
```

### å¯¹æ¯”æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash

models=("gpt2" "distilgpt2")
config="medium_scale"

# åˆ›å»ºé…ç½®
python config.py --create $config --output ${config}_config.json

# æµ‹è¯•æ¯ä¸ªæ¨¡å‹
for model in "${models[@]}"; do
    echo "Testing $model..."
    python benchmark.py --model $model --config-file ${config}_config.json --output-file ${model//\//_}_${config}
    
    # åˆ†æç»“æœ
    python analyze_results.py ${model//\//_}_${config}.json --output-dir ${model//\//_}_analysis
done

echo "æ‰€æœ‰æ¨¡å‹æµ‹è¯•å®Œæˆï¼"
```

## è´¡çŒ®å’Œåé¦ˆ

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿ï¼š

1. æäº¤ Issue æè¿°é—®é¢˜
2. æäº¤ Pull Request è´¡çŒ®ä»£ç 
3. åˆ†äº«æ‚¨çš„æµ‹è¯•ç»“æœå’Œç»éªŒ

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ LICENSE æ–‡ä»¶ã€‚